{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to prepare labelled dataset from audio transcription files. For each audio file, transcription file is saved. From each transcript file, labelled dataset is generated. The labelled dataset contains a group of three sentences with their respective sentiment, keywords and trends. All the labelled dataset is stored separately for each file in output folder named \"label_data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to generate labelled dataset for sentiment, keywords and trends on transcripts generated and store in output folder\n",
    "import os\n",
    "import json\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize Hugging Face's sentiment analysis pipeline with CUDA if available\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"  # Pretrained sentiment model (fine-tuned on SST-2 dataset)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load sentiment pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# Download NLTK data (for sentence tokenization)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize RAKE for Keyword Extraction\n",
    "rake = Rake()\n",
    "\n",
    "# Function to load and process files from the transcript folder\n",
    "def process_transcripts(folder_path, output_folder, sentence_group_size=3):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        \n",
    "    labeled_data = []\n",
    "    \n",
    "    # Loop through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):  \n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            output_file = os.path.join(output_folder, f\"labeled_{filename}.json\")\n",
    "            \n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                \n",
    "                # Split the content into sentences\n",
    "                sentences = sent_tokenize(content)\n",
    "                \n",
    "                # Process each sentence\n",
    "                file_labeled_data = []\n",
    "                sentence_groups = [sentences[i:i+sentence_group_size] for i in range(0, len(sentences), sentence_group_size)]\n",
    "                \n",
    "                for group in sentence_groups:\n",
    "                    sentiment_label = get_sentiment(' '.join(group))  # Perform sentiment analysis on the whole group\n",
    "                    keywords = extract_keywords(' '.join(group))  # Extract keywords for the whole group\n",
    "                    trends = extract_trends_using_lda(group)  # Extract trends for the whole group\n",
    "                    \n",
    "                    # for sentence in group:\n",
    "                    #     # Associate the same sentiment, keywords, and trends with each sentence in the group\n",
    "                    file_labeled_data.append({\n",
    "                        'sentence': group,\n",
    "                        'sentiment': sentiment_label,\n",
    "                        'keywords': keywords,\n",
    "                        'trends': trends\n",
    "                    })\n",
    "                \n",
    "                # Save the labeled data for the current file\n",
    "                save_labeled_data(file_labeled_data, output_file)\n",
    "\n",
    "# Function to get sentiment label (positive, negative, neutral)\n",
    "def get_sentiment(text, max_length=512):\n",
    "    # Tokenize the text to measure its token length\n",
    "    tokenized_text = tokenizer(text, truncation=False, return_tensors=\"pt\")\n",
    "    num_tokens = tokenized_text.input_ids.shape[1]\n",
    "    \n",
    "    if num_tokens > max_length:\n",
    "        # Split the text into smaller chunks\n",
    "        chunks = []\n",
    "        chunk_size = max_length - 10  # Allow space for special tokens like [CLS] and [SEP]\n",
    "        for i in range(0, num_tokens, chunk_size):\n",
    "            chunk = text[i:i+chunk_size]\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # Analyze each chunk separately and aggregate results\n",
    "        sentiments = []\n",
    "        for chunk in chunks:\n",
    "            result = sentiment_analyzer(chunk)[0]\n",
    "            sentiments.append(result['label'])\n",
    "        \n",
    "        # Determine final sentiment based on majority voting\n",
    "        final_sentiment = max(set(sentiments), key=sentiments.count)\n",
    "    else:\n",
    "        # If the text length is within limits, analyze directly\n",
    "        result = sentiment_analyzer(text)[0]\n",
    "        final_sentiment = result['label']\n",
    "    \n",
    "    # Map the label to a human-readable sentiment\n",
    "    if final_sentiment == 'POSITIVE':\n",
    "        return 'positive'\n",
    "    elif final_sentiment == 'NEGATIVE':\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Function to extract keywords using RAKE and get top 5 keywords using RAKE Score\n",
    "def extract_keywords(sentence, top_n=7):\n",
    "    # Extract keywords with scores\n",
    "    rake.extract_keywords_from_text(sentence)\n",
    "    keyword_scores = rake.get_ranked_phrases_with_scores()\n",
    "    \n",
    "    # Sort keywords by score (descending order) and select top N\n",
    "    sorted_keywords = sorted(keyword_scores, key=lambda x: x[0], reverse=True)\n",
    "    top_keywords = [keyword for score, keyword in sorted_keywords[:top_n]]\n",
    "    \n",
    "    return top_keywords\n",
    "\n",
    "# Function to preprocess and clean the text\n",
    "def preprocess_text(text, min_length=3):\n",
    "    # Tokenize the sentence and remove stopwords\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "    # If the sentence after filtering has fewer than min_length tokens, skip it\n",
    "    if len(filtered_tokens) < min_length:\n",
    "        return \"\"\n",
    "    \n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "\n",
    "# Function to train the LDA model and extract trends\n",
    "def extract_trends_using_lda(sentences, num_topics=5, top_n = 7):\n",
    "    # Preprocess all sentences\n",
    "    processed_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "    \n",
    "    # Filter out empty sentences after preprocessing\n",
    "    processed_sentences = [sentence for sentence in processed_sentences if sentence.strip()]\n",
    "    \n",
    "    if len(processed_sentences) == 0:\n",
    "        return []  # If no valid sentences remain, return an empty list of trends\n",
    "\n",
    "    # Convert the sentences into a Term-Document matrix using CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(processed_sentences)\n",
    "\n",
    "    # Train the LDA model\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda.fit(X)\n",
    "\n",
    "    # Get the top words for each topic with their relevance scores\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topic_scores = []\n",
    "    \n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words_idx = topic.argsort()[-10:][::-1]  # Top 10 words for this topic\n",
    "        top_words = [(feature_names[i], topic[i]) for i in top_words_idx]\n",
    "        topic_scores.extend(top_words)\n",
    "    \n",
    "    # Aggregate trends based on their scores\n",
    "    word_scores = Counter()\n",
    "    for word, score in topic_scores:\n",
    "        word_scores[word] += score\n",
    "    \n",
    "    # Sort by relevance and return the top N trends\n",
    "    sorted_trends = [word for word, score in word_scores.most_common(top_n)]\n",
    "    return sorted_trends\n",
    "\n",
    "\n",
    "# Save labeled dataset as JSON\n",
    "def save_labeled_data(labeled_data, output_file):\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(labeled_data, json_file, indent=4)\n",
    "        \n",
    "    print(f\"Labeled data saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"/Audio/Rapaport Podcasts/Transcripts\"\n",
    "output_folder = \"/Audio/Rapaport Podcasts/label_data\"\n",
    "\n",
    "#process the same step for 2nd folder with path as \n",
    "# folder_path = \"/Audio/Youtube market Analysis/Transcripts\"\n",
    "# output_folder = \"/Audio/Youtube market Analysis/label_data\"\n",
    "\n",
    "# Process the transcripts and generate labeled data\n",
    "process_transcripts(folder_path, output_folder)\n",
    "\n",
    "print(\"Process complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
