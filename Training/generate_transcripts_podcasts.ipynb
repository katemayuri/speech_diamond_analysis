{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to generate transcript for each audio file saved in \"Rapaport Podcasts\" folder and saved into \"Transcripts\" folder. Transcription is performed by using OpenAI's whisper medium model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code which has generated transcripts for 50 rapaport podcasta .mp3 files and saved into output folder\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "# Initialize Whisper pipeline for transcription\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "transcriber = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-medium\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Paths\n",
    "audio_folder = \"/Audio/Rapaport Podcasts/Audio\"\n",
    "output_folder = \"/Audio/Rapaport Podcasts/Transcripts\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to process audio and transcribe directly\n",
    "def transcribe_audio(audio_path, chunk_length_ms=30000):\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    full_transcription = \"\"\n",
    "    \n",
    "    # Process each chunk one at a time\n",
    "    for i in range(0, len(audio), chunk_length_ms):\n",
    "        chunk = audio[i:i + chunk_length_ms]\n",
    "        temp_file = f\"temp_chunk.wav\"\n",
    "        chunk.export(temp_file, format=\"wav\")\n",
    "        \n",
    "        # Transcribe the chunk\n",
    "        result = transcriber(temp_file)\n",
    "        full_transcription += result[\"text\"] + \" \"\n",
    "        \n",
    "        # Remove the temporary file\n",
    "        os.remove(temp_file)\n",
    "    \n",
    "    return full_transcription\n",
    "\n",
    "# Process each audio file in the folder\n",
    "for k, filename in enumerate(os.listdir(audio_folder)):\n",
    "    if filename.endswith(\".mp3\") or filename.endswith(\".wav\"):\n",
    "        audio_path = os.path.join(audio_folder, filename)\n",
    "        \n",
    "        # Transcribe the audio file\n",
    "        try:\n",
    "            transcription = transcribe_audio(audio_path)\n",
    "            output_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "            \n",
    "            # Save transcription to text file\n",
    "            with open(output_path, \"w\") as f:\n",
    "                f.write(transcription)\n",
    "            \n",
    "            # Print status\n",
    "            print(f\"{k + 1} - File Transcription done for {filename}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "print(\"Rapaport Podcasts Transcriptions completed and saved to text files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
